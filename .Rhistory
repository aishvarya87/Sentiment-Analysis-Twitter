summary(rf_mod)
index1 <- sample(1:nrow(df_rf),200,replace = F)
names(df_rf)
smp_rf <- df_rf[index1,c(5,6)]
# Score a new data
NewPredictions <- predict(rf_mod, smp_rf)
index1 <- sample(1:nrow(df_rf),200,replace = F)
names(df_rf)
smp_rf <- df_rf[index1,c(19,21)]
# Score a new data
NewPredictions <- predict(rf_mod, smp_rf)
head(NewPredictions)
table(NewPredictions)
contents(NewPredictions)
print(NewPredictions)
index1 <- sample(1:nrow(df_rf),200,replace = F)
names(df_rf)
smp_rf <- df_rf[index1,c(19,21)]
# Score a new data
NewPredictions <- predict(rf_mod, smp_rf)
head(NewPredictions)
table(NewPredictions)
print(NewPredictions)
index1 <- sample(1:nrow(df_rf),100,replace = F)
names(df_rf)
smp_rf <- df_rf[index1,c(19,21)]
# Score a new data
NewPredictions <- predict(rf_mod, smp_rf)
head(NewPredictions)
table(NewPredictions)
print(NewPredictions)
#####START FROM HERE######################################################################################
setwd("C:\\Users\\Aishvarya\\Desktop\\R analysis\\Sentiment Analysis")
getwd()
if (!require('pacman')) install.packages('pacman')
pacman::p_load(twitteR, syuzhet, plyr, ggplot2, wordcloud, RColorBrewer,
httpuv, RCurl, openssl, base64enc, foreign, lubridate, tm, Rstem)
options(RCurlOptions = list(cainfo = system.file('CurlSSL', 'cacert.pem', package = 'RCurl')))
if (!require('pacman')) install.packages('pacman&')
pacman::p_load(devtools, installr)
install.Rtools()
install_url('http://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz')
install_url('http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz')
api_key = 'rzQLiaeBfDzXTiYZbdhF0fiHM'
api_secret = 'Tcc13D6083iCXd1nEK0wHf4PZGbm8XtgaTRNPNwgTuxXGXzjIn'
access_token = '103206999-6UwDo8kaSGXFS4gKlAhMNiHskqrHhxFlc64li2sl'
access_token_secret = 'wsyoXIyggZL04mRQoLZVjY8DxRgJD80HPAprEtcXXuh70'
setup_twitter_oauth(api_key,api_secret, access_token, access_token_secret)
# get the text
some_txt = sapply(some_tweets, function(x) x$getText())
# remove retweet entities
some_txt = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', some_txt)
# remove at people
some_txt = gsub('@\\w+', '', some_txt)
# harvest some tweets
some_tweets = searchTwitter('GalaxyS9', n=5000, lang='en')
# get the text
some_txt = sapply(some_tweets, function(x) x$getText())
# remove retweet entities
some_txt = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', some_txt)
# remove at people
some_txt = gsub('@\\w+', '', some_txt)
# remove punctuation
some_txt = gsub('[[:punct:]]', '', some_txt)
# remove numbers
some_txt = gsub('[[:digit:]]', '', some_txt)
# remove html links
some_txt = gsub('http\\w+', '', some_txt)
# remove unnecessary spaces
some_txt = gsub('[ \t]{2,}', '', some_txt)
some_txt = gsub('^\\s+|\\s+$', '', some_txt)
# define 'tolower error handling' function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
classify_emotion <- function(textColumns,algorithm="bayes",prior=1.0,verbose=FALSE,...) {
matrix <- create_matrix(textColumns,...)
lexicon <- read.csv(system.file("data/emotions.csv.gz",package="sentiment"),header=FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
return(documents)
}
create_matrix <- function(textColumns, language="english", minDocFreq=1, minWordLength=3, removeNumbers=TRUE, removePunctuation=TRUE, removeSparseTerms=0, removeStopwords=TRUE, stemWords=FALSE, stripWhitespace=TRUE, toLower=TRUE, weighting=weightTf) {
stem_words <- function(x) {
split <- strsplit(x," ")
return(wordStem(split[[1]],language=language))
}
control <- list(language=language,tolower=toLower,removeNumbers=removeNumbers,removePunctuation=removePunctuation,stripWhitespace=stripWhitespace,minWordLength=minWordLength,stopwords=removeStopwords,minDocFreq=minDocFreq,weighting=weighting)
if (stemWords == TRUE) control <- append(control,list(stemming=stem_words),after=6)
trainingColumn <- apply(as.matrix(textColumns),1,paste,collapse=" ")
trainingColumn <- sapply(as.vector(trainingColumn,mode="character"),iconv,to="UTF8",sub="byte")
corpus <- Corpus(VectorSource(trainingColumn),readerControl=list(language=language))
matrix <- DocumentTermMatrix(corpus,control=control);
if (removeSparseTerms > 0) matrix <- removeSparseTerms(matrix,removeSparseTerms)
gc()
return(matrix)
}
classify_polarity <- function(textColumns,algorithm="bayes",pstrong=0.5,pweak=1.0,prior=1.0,verbose=FALSE,...) {
matrix <- create_matrix(textColumns,...)
lexicon <- read.csv(system.file("data/subjectivity.csv.gz",package="sentiment"),header=FALSE)
counts <- list(positive=length(which(lexicon[,3]=="positive")),negative=length(which(lexicon[,3]=="negative")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(positive=0,negative=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
index <- pmatch(word,lexicon[,1],nomatch=0)
if (index > 0) {
entry <- lexicon[index,]
polarity <- as.character(entry[[2]])
category <- as.character(entry[[3]])
count <- counts[[category]]
score <- pweak
if (polarity == "strongsubj") score <- pstrong
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"POL:",polarity,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
ratio <- as.integer(abs(scores$positive/scores$negative))
if (ratio==1) best_fit <- "neutral"
documents <- rbind(documents,c(scores$positive,scores$negative,abs(scores$positive/scores$negative),best_fit))
if (verbose) {
print(paste("POS:",scores$positive,"NEG:",scores$negative,"RATIO:",abs(scores$positive/scores$negative)))
cat("\n")
}
}
colnames(documents) <- c("POS","NEG","POS/NEG","BEST_FIT")
return(documents)
}
# Perform Sentiment Analysis
# classify emotion
class_emo = classify_emotion(some_txt, algorithm='bayes', prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by 'unknown'
emotion[is.na(emotion)] = 'unknown'
# classify polarity
class_pol = classify_polarity(some_txt, algorithm='bayes')
# get polarity best fit
polarity = class_pol[,4]
# Create data frame with the results and obtain some general statistics
# data frame with results
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
# sort data frame
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
# Let's do some plots of the obtained results
# plot distribution of emotions
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='emotion categories', y='number of tweets') +
ggtitle('Sentiment Analysis of Tweets about GalaxyS9\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))
# plot distribution of polarity
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette='RdGy') +
labs(x='polarity categories', y='number of tweets') +
ggtitle('Sentiment Analysis of Tweets about GalaxyS9\n(classification by polarity)') +
theme(plot.title = element_text(size=12, face='bold'))
# Separate the text by emotions and visualize the words with a comparison cloud
# separating text by emotion
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep('', nemo)
for (i in 1:nemo)
{
tmp = some_txt[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=' ')
}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords('english'))
# create corpus
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
head(tdm, 20)
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(nemo, 'Dark2'),
scale = c(3,.5), random.order = FALSE, title.size = 2)
v <- sort(rowSums(tdm),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
write.csv("Twitter_Output.csv",d)
# Generate the WordCloud
library("wordcloud")
library("RColorBrewer")
par(bg="#000000")
png(file="Keywords.png",width=500,height=400, bg="#000000")
wordcloud(d$word, d$freq, col=terrain.colors(length(d$word), alpha=0.9), random.order=FALSE, rot.per=0.3,max.words = 150)
title(main = "Most Used Keywords in the Tweets", font.main = 1, col.main = "cornsilk4", cex.main = 1.5)
dev.off()
#####START FROM HERE######################################################################################
setwd("C:\\Users\\Aishvarya\\Desktop\\R analysis\\Sentiment Analysis")
getwd()
if (!require('pacman')) install.packages('pacman')
pacman::p_load(twitteR, syuzhet, plyr, ggplot2, wordcloud, RColorBrewer,
httpuv, RCurl, openssl, base64enc, foreign, lubridate, tm, Rstem)
options(RCurlOptions = list(cainfo = system.file('CurlSSL', 'cacert.pem', package = 'RCurl')))
if (!require('pacman')) install.packages('pacman&')
pacman::p_load(devtools, installr)
install.Rtools()
install_url('http://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz')
install_url('http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz')
api_key = 'rzQLiaeBfDzXTiYZbdhF0fiHM'
api_secret = 'Tcc13D6083iCXd1nEK0wHf4PZGbm8XtgaTRNPNwgTuxXGXzjIn'
access_token = '103206999-6UwDo8kaSGXFS4gKlAhMNiHskqrHhxFlc64li2sl'
access_token_secret = 'wsyoXIyggZL04mRQoLZVjY8DxRgJD80HPAprEtcXXuh70'
setup_twitter_oauth(api_key,api_secret, access_token, access_token_secret)
# harvest some tweets
some_tweets = searchTwitter('GalaxyS9', n=5000, lang='en')
# get the text
some_txt = sapply(some_tweets, function(x) x$getText())
# remove retweet entities
some_txt = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', some_txt)
# remove at people
some_txt = gsub('@\\w+', '', some_txt)
# remove punctuation
some_txt = gsub('[[:punct:]]', '', some_txt)
# remove numbers
some_txt = gsub('[[:digit:]]', '', some_txt)
# remove html links
some_txt = gsub('http\\w+', '', some_txt)
# remove unnecessary spaces
some_txt = gsub('[ \t]{2,}', '', some_txt)
some_txt = gsub('^\\s+|\\s+$', '', some_txt)
# define 'tolower error handling' function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
word<-get_nrc_sentiment(some_txt)
td<-data.frame(t(word))
td_new <- data.frame(rowSums(td[2:5000]))
#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
library("ggplot2")
qplot(sentiment, data=td_new2, weight=count, geom="bar",fill=sentiment)+ggtitle("Sentiments Expressed for GalaxyS9 on Twitter - 3/2/2018")
classify_emotion <- function(textColumns,algorithm="bayes",prior=1.0,verbose=FALSE,...) {
matrix <- create_matrix(textColumns,...)
lexicon <- read.csv(system.file("data/emotions.csv.gz",package="sentiment"),header=FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
return(documents)
}
create_matrix <- function(textColumns, language="english", minDocFreq=1, minWordLength=3, removeNumbers=TRUE, removePunctuation=TRUE, removeSparseTerms=0, removeStopwords=TRUE, stemWords=FALSE, stripWhitespace=TRUE, toLower=TRUE, weighting=weightTf) {
stem_words <- function(x) {
split <- strsplit(x," ")
return(wordStem(split[[1]],language=language))
}
control <- list(language=language,tolower=toLower,removeNumbers=removeNumbers,removePunctuation=removePunctuation,stripWhitespace=stripWhitespace,minWordLength=minWordLength,stopwords=removeStopwords,minDocFreq=minDocFreq,weighting=weighting)
if (stemWords == TRUE) control <- append(control,list(stemming=stem_words),after=6)
trainingColumn <- apply(as.matrix(textColumns),1,paste,collapse=" ")
trainingColumn <- sapply(as.vector(trainingColumn,mode="character"),iconv,to="UTF8",sub="byte")
corpus <- Corpus(VectorSource(trainingColumn),readerControl=list(language=language))
matrix <- DocumentTermMatrix(corpus,control=control);
if (removeSparseTerms > 0) matrix <- removeSparseTerms(matrix,removeSparseTerms)
gc()
return(matrix)
}
classify_polarity <- function(textColumns,algorithm="bayes",pstrong=0.5,pweak=1.0,prior=1.0,verbose=FALSE,...) {
matrix <- create_matrix(textColumns,...)
lexicon <- read.csv(system.file("data/subjectivity.csv.gz",package="sentiment"),header=FALSE)
counts <- list(positive=length(which(lexicon[,3]=="positive")),negative=length(which(lexicon[,3]=="negative")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(positive=0,negative=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
index <- pmatch(word,lexicon[,1],nomatch=0)
if (index > 0) {
entry <- lexicon[index,]
polarity <- as.character(entry[[2]])
category <- as.character(entry[[3]])
count <- counts[[category]]
score <- pweak
if (polarity == "strongsubj") score <- pstrong
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"POL:",polarity,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
ratio <- as.integer(abs(scores$positive/scores$negative))
if (ratio==1) best_fit <- "neutral"
documents <- rbind(documents,c(scores$positive,scores$negative,abs(scores$positive/scores$negative),best_fit))
if (verbose) {
print(paste("POS:",scores$positive,"NEG:",scores$negative,"RATIO:",abs(scores$positive/scores$negative)))
cat("\n")
}
}
colnames(documents) <- c("POS","NEG","POS/NEG","BEST_FIT")
return(documents)
}
# Perform Sentiment Analysis using classify_emotion - IGNORE
# classify emotion
class_emo = classify_emotion(some_txt, algorithm='bayes', prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by 'unknown'
emotion[is.na(emotion)] = 'unknown'
# classify polarity
class_pol = classify_polarity(some_txt, algorithm='bayes')
# get polarity best fit
polarity = class_pol[,4]
# Create data frame with the results and obtain some general statistics
# data frame with results
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
# sort data frame
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
# Let's do some plots of the obtained results
# plot distribution of emotions
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='emotion categories', y='number of tweets') +
ggtitle('Sentiment Analysis of Tweets about GalaxyS9\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))
# plot distribution of polarity
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette='RdGy') +
labs(x='polarity categories', y='number of tweets') +
ggtitle('Sentiment Analysis of Tweets about GalaxyS9\n(classification by polarity)') +
theme(plot.title = element_text(size=12, face='bold'))
# Separate the text by emotions and visualize the words with a comparison cloud
# separating text by emotion
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep('', nemo)
for (i in 1:nemo)
{
tmp = some_txt[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=' ')
}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords('english'))
# create corpus
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
head(tdm, 20)
# comparison word cloud - IGNORE
comparison.cloud(tdm, colors = brewer.pal(nemo, 'Dark2'),
scale = c(3,.5), random.order = FALSE, title.size = 2)
v <- sort(rowSums(tdm),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
write.csv("Twitter_Output.csv",d)
v <- sort(rowSums(tdm),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
write.csv("Twitter_Output.csv",d)
# Generate the WordCloud
library("wordcloud")
library("RColorBrewer")
# Generate the WordCloud
library("wordcloud")
library("RColorBrewer")
par(bg="#000000")
png(file="Keywords.png",width=500,height=400, bg="#000000")
wordcloud(d$word, d$freq, col=terrain.colors(length(d$word), alpha=0.9), random.order=FALSE, rot.per=0.3,max.words = 150)
title(main = "Most Used Keywords in the Tweets", font.main = 1, col.main = "cornsilk4", cex.main = 1.5)
dev.off()
